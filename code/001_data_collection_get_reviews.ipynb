{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "\n",
    "import urllib2\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from __future__ import division\n",
    "\n",
    "proxy = urllib2.ProxyHandler({'http': 'http://username:password@proxy.indra.es:8080/'})\n",
    "opener = urllib2.build_opener(proxy)\n",
    "urllib2.install_opener(opener)\n",
    "\n",
    "##\n",
    "use_proxy='YES'\n",
    "path=\"/media/jupiter/hadoopuser/amazon/data/\"\n",
    "##\n",
    "pull_ids='NO'\n",
    "get_meta_info='YES'\n",
    "scrape_meta_info='YES'\n",
    "if use_proxy=='NO':\n",
    "    opener = urllib2.build_opener()\n",
    "    urllib2.install_opener(opener)\n",
    "\n",
    "import requests\n",
    "\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#change to category name of items scraping\n",
    "#if change from drill, need to change text filter in line 198\n",
    "category_name=\"drill\"\n",
    "fd = open(path + \"meta_\" + category_name + \"_final\" + \".json\", \"r\")\n",
    "text=fd.read()\n",
    "meta_info  = json.loads(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Get Reviews####\n",
    "AMAZON_URL = 'http://www.amzn.com/product-reviews/'\n",
    "ref1='/ref=cm_cr_pr_viewopt_srt?ie=UTF8&pageNumber='\n",
    "ref2='&sortBy=recent&reviewerType=all_reviews&filterByStar=all_stars'\n",
    "#url = AMAZON_URL + asin +ref1 + str(pageNo) +ref2\n",
    "\n",
    "#Grabs soup for the nth page of reviews order by date\n",
    "def get_soup(asin,pageNo):\n",
    "    opener = urllib2.build_opener()\n",
    "    opener.addheaders = [('User-agent', 'Chrome/42.0.2311.90')]\n",
    "    soup_info={}\n",
    "    url = AMAZON_URL + asin +ref1 + str(pageNo) +ref2\n",
    "    r = opener.open(url)    \n",
    "    s = BeautifulSoup(r.read())   \n",
    "    soup_info[\"asin\"] = asin\n",
    "    soup_info[\"url\"] = url\n",
    "    soup_info[\"soup\"]= s\n",
    "    soup_info[\"pageNo\"] = pageNo\n",
    "    return soup_info\n",
    "\n",
    "#gets information from the soup object\n",
    "def get_review_info(s,list_name):\n",
    "    totReviews=len(s[\"soup\"].find_all(class_=\"a-section review\"))\n",
    "    listReviews= s[\"soup\"].find_all(class_=\"a-section review\")\n",
    "    if s[\"pageNo\"]==1:\n",
    "       \n",
    "        pages[\"asin\"]=s[\"asin\"]\n",
    "        pages[\"noReviewPages\"]=str(s[\"soup\"].find_all(class_=\"page-button\")[len(s[\"soup\"].find_all(class_=\"page-button\"))-1].get_text())\n",
    "        asin_pages.append(pages)\n",
    "    for R in listReviews:\n",
    "        rev={}\n",
    "        rev[\"asin\"]=s[\"asin\"]\n",
    "        rev[\"reviewPageNo\"]=s[\"pageNo\"]\n",
    "        rev[\"reviewText\"]=R.find_all(class_=\"a-row review-data\")[0].get_text()\n",
    "        rev[\"reviewDate\"]=R.find_all(class_=re.compile(\"review-date\"))[0].get_text().replace(\"on\",\"\")\n",
    "        rev[\"reviewId\"]=R.get(\"id\")\n",
    "        try:\n",
    "            rev[\"reviewAuthor\"]=R.find_all(class_=re.compile(\"author\"))[0].get_text()\n",
    "        except:\n",
    "            rev[\"reviewAuthor\"]='Unknown'\n",
    "        rev[\"reviewTitle\"]=R.find_all(class_=re.compile(\"review-title\"))[0].get_text()\n",
    "        rev[\"reviewStars\"]=R.find_all(class_=re.compile(\"a-icon-alt\"))[0].get_text()\n",
    "        try:\n",
    "            rev[\"reviewVerified\"]=R.find_all(class_=\"a-size-mini a-color-state a-text-bold\")[0].get_text()\n",
    "        except:\n",
    "            rev[\"reviewVerified\"]=\"Not verified\"    \n",
    "        try:\n",
    "            rev[\"helpfulCount\"]=R.find_all(class_=\"a-row helpful-votes-count\")[0].get_text()\n",
    "        except:\n",
    "            rev[\"helpfulCount\"]=0\n",
    "        list_name.append(rev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Read in product ids and number of review pages ####\n",
    "##get asin id and number of review pages for each product\n",
    "import math\n",
    "import pandas as pd\n",
    "reviews=[]\n",
    "asin_pages=[]\n",
    "\n",
    "for item in meta_info:\n",
    "    pages={}\n",
    "    pages[\"noReviewPages\"]=math.ceil(int(item[\"numOfReviews\"])/10)\n",
    "    pages[\"asin\"]=item[\"asin\"]\n",
    "    asin_pages.append(pages)\n",
    "    \n",
    "   \n",
    "asinPages=pd.DataFrame(asin_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##order ids by fewest number of review pages\n",
    "asinPages=asinPages.sort((\"noReviewPages\",\"asin\"))\n",
    "asinList=[items for items in asinPages.ix[:,0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_list=[]\n",
    "from random import randint\n",
    "\n",
    "for idx,items in enumerate(missing):\n",
    "    print(items)   \n",
    "    with open(path +\"reviews_drill_\"+ str(items) +\".json\", \"w\") as fd:\n",
    "        pages={}\n",
    "        #Block the line below out if process stalls in the middle of one product's downloads \n",
    "        #and change the range to the page stalled so you don't have to rerun entire product again\n",
    "        reviews2=[]    \n",
    "        soup_info=get_soup(items,1)\n",
    "        info=get_review_info(soup_info, reviews2) \n",
    "        pageNo=int(pages[\"noReviewPages\"])\n",
    "        print(pageNo)\n",
    "        for i in range(2,pageNo+1):\n",
    "            soup_info=get_soup(items,i)\n",
    "            info=get_review_info(soup_info, reviews2)\n",
    "            time.sleep(randint(10,15))        \n",
    "            if i % 7 ==0 and idx !=0:  \n",
    "                print(i)\n",
    "                time.sleep(5)\n",
    "        fd.write(json.dumps(reviews2)) \n",
    "        print(str(i)+' pages scraped')\n",
    "        print(len(reviews2))       \n",
    "        time.sleep(20)\n",
    "        if idx % 30 ==0 and idx !=0:\n",
    "                print(idx)\n",
    "                time.sleep(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################## TROUBLESHOOTING ##########################\n",
    "###use only to dedup lists that need to be reran\n",
    "'''\n",
    "import operator\n",
    "len(set([item[\"reviewId\"] for item in reviews2]))\n",
    "\n",
    "reviewId=\n",
    "\n",
    "fd=open(path +\"reviews_drill_\" + review_id + \".json\", \"r\")\n",
    "text = fd.read()\n",
    "\n",
    "pmeta = json.loads(text)\n",
    "len(pmeta)\n",
    "\n",
    "from operator import itemgetter\n",
    "newlist = sorted(pmeta, key=itemgetter('reviewId'))\n",
    "\n",
    "last_id=\"\"\n",
    "newlist2=[]\n",
    "for item in newlist:\n",
    "    if item[\"reviewId\"] != last_id:\n",
    "        newlist2.append(item)    \n",
    "        last_id=item[\"reviewId\"]\n",
    "        \n",
    "with open(path +\"reviews_drill_\" + review_id + \".json\", \"w\") as f:\n",
    "    f.write(json.dumps(newlist2)) \n",
    "\n",
    "'''\n",
    "'''\n",
    "####if you missed a file and need to check that you have them all:\n",
    "import os\n",
    "import re\n",
    "finished=[re.findall(\"B[0-9]{2}[0-9A-Z]{7}\",filename)[0] for filename \\\n",
    " in os.listdir(path) if filename.find('reviews_drill_B')>-1]\n",
    "\n",
    "\n",
    "missing= [i for i in asinList if i not in finished]\n",
    "len(finished)\n",
    "missing\n",
    "'''\n",
    "\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15130"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################DUMP ALL REVIEWS INTO JSON FILE ########\n",
    "\n",
    "import os\n",
    "import re\n",
    "finished=[re.findall(\"B[0-9]{2}[0-9A-Z]{7}\",filename)[0] for filename \\\n",
    "     in os.listdir(path) if filename.find('reviews_drill_B')>-1]\n",
    "\n",
    "#read in all individual product reviews json files\n",
    "reviews=[]\n",
    "for id in finished:\n",
    "    fd = open(path+\"reviews_drill_\" + id + \".json\", \"r\")\n",
    "    text = fd.read()\n",
    "    pmeta = json.loads(text)\n",
    "    reviews = reviews + pmeta\n",
    "    fd.close()\n",
    "\n",
    "#Write all reviews out to file    \n",
    "reviews2 = json.dumps(obj = reviews,indent = 4)\n",
    "fd = open(path+category_name+\"_reviews_\" + time.strftime(\"%Y%m%d\") + \".json\", \"w\")\n",
    "fd.write(reviews2)\n",
    "fd.close()\n",
    "\n",
    "len(reviews)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
